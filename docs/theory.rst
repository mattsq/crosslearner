Theoretical Background
======================

AC-X builds on two key ideas in modern causal inference: the X-learner
meta-approach and the DragonNet architecture.  The X-learner imputes
counterfactual outcomes from separate models for the treated and
control groups, producing pseudo-outcomes that can be used to learn a
final treatment effect estimator.  DragonNet in contrast trains a
single neural network with shared representations and an auxiliary
propensity head.  Its objective ties the potential outcome heads
together through regularisation based on treatment probability.

AC-X combines these perspectives.  The network resembles a simplified
DragonNet with explicit potential outcome heads and a separate
treatment-effect head.  It still follows the X-learner philosophy of
borrowing the opposite armâ€™s prediction as a pseudo-outcome, but all
components are trained jointly so that gradients from the
consistency and adversarial terms update both outcome heads and the
shared representation.

Benefits of Adversarial Training
--------------------------------

Enforcing an adversarial game between a generator (the potential
outcome models) and a discriminator helps the model learn
counterfactuals that are indistinguishable from real observations.
This adversarial signal provides useful gradients even when overlap is
limited, acting as an implicit regulariser and reducing bias from
covariate imbalance.  Empirically, adversarial training tends to lower
root PEHE and encourages smoother treatment-effect surfaces.

AC-X Training Procedure
-----------------------

The following pseudocode sketches the training loop implemented in
:mod:`crosslearner.training.train_acx`.

.. code-block:: text

   initialise AC-X model and optimisers
   for each epoch:
       for each batch (X, T, Y):
           # update discriminator
           h, m0, m1, _ = model(X)
           Y_cf = where(T == 1, m0, m1)
           real_logits = discriminator(h, Y, T)
           fake_logits = discriminator(h, Y_cf, T)
           loss_d = adv_loss(real_logits, fake_logits)
           backprop and update discriminator

           # update generator
           h, m0, m1, tau = model(X)
           m_obs = where(T == 1, m1, m0)
           loss_y = mse(m_obs, Y)
           loss_cons = mse(tau, m1 - m0)
           Y_cf = where(T == 1, m0, m1)
           fake_logits = discriminator(h, Y_cf, T)
           loss_adv = generator_adv_loss(fake_logits)
           loss_g = alpha_out * loss_y + beta_cons * loss_cons + gamma_adv * loss_adv
           backprop and update generator

   return trained model

Noise Injection and Consistency
-------------------------------

To encourage robustness AC-X can optionally enforce consistency under
small input perturbations. During training each batch may be augmented
with Gaussian noise ``\epsilon \sim \mathcal{N}(0, \text{noise\_std}^2)`` and
the model predictions on ``X + \epsilon`` are required to match those on the
original ``X``. The additional mean squared error term is weighted by
``noise_consistency_weight`` and applied to both potential outcome heads and
the treatment effect head.

Representation Drift Regularization
----------------------------------

Training GANs can suffer from unstable feature representations.  AC-X includes
an optional term to discourage large shifts in the encoder between epochs.
For each treatment group the mean and variance of the representation are
tracked using an exponential moving average controlled by ``rep_momentum``.
During training the current batch statistics are penalised against the stored
averages weighted by ``rep_consistency_weight``.

